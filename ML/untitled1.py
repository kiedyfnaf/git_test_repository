# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W8N8MWFVxJ2cL3OP76Li87LL7evUUppL

# Begginings
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

scalar = torch.tensor(7)
scalar

scalar.ndim

scalar.item()

vector = torch.tensor([7, 7])
vector

vector.ndim

vector.shape

MATRIX = torch.tensor([[7, 8],
                       [9, 10]])
MATRIX

MATRIX.ndim

MATRIX[1]

MATRIX.shape

#Tensor

TENSOR = torch.tensor([[[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]]])
TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR[0]

# Random tensor

random_tensor = torch.rand(2, 3, 4)
random_tensor

random_image_size_tensor = torch.rand(size=(3, 224, 224))
random_image_size_tensor.shape, random_image_size_tensor.ndim

#Zero tensor

zeros = torch.zeros(size=(3, 4))
zeros * random_tensor

ones = torch.ones(size=(3, 4))
ones * random_tensor

zeros.dtype

#Range of tensors

torch.arange(0, 10) #both kinda do the same, better to use @!!!arange!!!@
torch.range(0, 10)

step = torch.arange(start=0, end=1000, step=17)

#Tensors like

ten_zeros = torch.zeros_like(input=step)
ten_zeros

"""# Datatype

"""

#Tensor datatype

float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None, # What datatype (float16, 32...)
                               device = None, #What your device is on
                               requires_grad=False) #Whether or not to track gradients with this tensors operations
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

float_16_tensor * float_32_tensor

#Get information from a tensor

"""# Problems / Operations


"""

#Tensors not right datatype - to do get datatype from a tensor, can use tensor.dtype
#Tensors not right shape - to get shape from a tensor, can use tensor.shape
#Tensors not on the right device - to get device from a tensor, can use tensor.device

float_32_tensor.device

some_tensor = torch.rand(3, 4)
some_tensor

###Tensor manipulations (operations) ###
#1. Addition
#2. Subtraction
#3. Multiplication (element-wise)
#4. Division
#5. Matrix multiplication

#Create a tensor and add
tensor = torch.tensor([1, 2, 3])
tensor + 10

#Multiply by 10
tensor * 10

#Subtract
tensor - 10

# Pytorch inbuild functions
torch.mul(tensor, 10)

### Matrix multiplication ###
#Two main ways to multiply
#1. Element-wise multiplication
#2. Matrix multiplication (dot product)

tensor * tensor #one is (1, 2, 3)

torch.matmul(tensor, tensor) # 1*1 + 2*2 + 3*3

### Shape errors ###
#1. Inner dimensions must match
#(3, 2) @ (3, 2) won't work
#(2, 3) @ (3, 2) will work  (inner = 3, 3)
#(3, 2) @ (2, 3) will work  (inner = 2, 2)
#The resulting matrix has the shape of the outer dimension
#(4, 2) @ (2, 3) = (4, 3)

#Shapes for matrix multiplication
tensor_A = torch.tensor([[1, 2],
                         [3, 4],
                         [5, 6]])
tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])
torch.matmul(tensor_A, tensor_B.T)

"""# Transposition, min, max, mean"""

# Transpose (swiches the axis of a given tensor)
tensor_B.T # T means transpose

torch.matmul(tensor_A.T, tensor_B) #(2, 3) @ (3, 2) = (2, 2)

# Charachteristics
x = torch.arange(1, 100, 10)
x

#Minimum
torch.min(x), x.min()

#Maximum
torch.max(x), x.max()

#Mean value
torch.mean(x) #doesn't work wrong dtype i have Long

x.dtype

torch.mean(x.type(torch.float32))

##Find the positional min and max

#Find the pos in tensor that has min gives position
x.argmin()

x[0]

#Find the max
x.argmax()

x[9]

"""# Reshaping, stacking, squeezing, and unsqueezing tensors, view"""

## Reshaping, stacking, squeezing, and unsqueezing tensors
#Reshaping - reshapes an input tensor to a defined shape
#View - return a view of an input tensor of certain shape but keep the same memory as the original tensor
# Stacking - combine multiple tensors on top of each other (wstack) or side by side (hstack)
#Squezze = removes all '1' dimensions from a tensor
#Unsqueeze = add a '1' dimension to a target tensor
#Permute = Return a view of the input with dimensions permuted (swapped) in a certain way

import torch
x = torch.arange(1., 10.)
x, x.shape

#Reshape , extra dim
x_reshaped = x.reshape(1, 7) # Error [1, 7] is invalid for 9 tensor
x_reshaped, x_reshaped(2, 9) # Error [2, 9] ,2 * 9 = 18 not 9
x_reshaped = x.reshape(1, 9)
x_reshaped, x_reshaped.shape

# Change the view
z = x.view(1, 9)
z, z.shape

# Changing z changes x (they have the same memory)
z[:, 0] = 5
z, x

#Stack tensors on top of each other
x_stacked = torch.stack([x, x, x, x], dim=0)
x_stacked

# Squezze tensors
x_reshaped.squeeze()

x_reshaped.squeeze().shape  #before was [1, 9]

#Unsqueeze, adds a single dim
x_unsqueezed = x_reshaped.squeeze()
x_unsqueezed.unsqueeze(dim=0)

x_unsqueezed.unsqueeze(dim=1)

x_unsqueezed.shape

"""# Permuted, indexing"""

#Permute rearrenges the dimensions
x_original = torch.rand(size=(224, 224, 3)) #[height, width, color_channels]
x_original

x_permuted = x_original.permute(2, 0, 1) #shifts axis 0->1, 1->2, 2->0
x_permuted

x_permuted.shape

x_original.shape #Now they also both share the same mamory so changing x_originl will affect x_permuted

### Indexing

import torch
x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape

x[0]

#Middle bracket
x[0][0], x[0, 0]

#Index on inner bracket (last dim)
x[0][0][0]

#Use # to select all of target dimension
x[:, 0]

#Get all value of oth and 1st dim but only index 1 of 2 dim
x[:,:,1]

#Get all values of the 0 dim but only 1 index value of 1st and 2nd dim
x[:,1,1]

#Get index 0 of 0th and 1st dim and all values of 2 dim
x[0,0,:]

# Index on x to return 9
x[0,2,2]

#Index on x to return 3, 6, 9
x[0,:,2]

"""# Pytorch tensors and NumPy Reproducibility (trying to take random out of random)"""

### Pytorch tensors and NumPy
# Data in NumPy, want in Pytorch tensor -> torch,from_numpy(ndarray)
#PyTorch tensor-> NumPy->torch.Tensor.numpy()

import torch
import numpy as np

array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array) # its float 64 base
array, tensor

array = array + 1
array, tensor

# Tensor to NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

### Reproducibility (trying to take random out of random)
# To reduce randomness in NN and Pytorch comes the concept of a random seed
# What is does is flavour randomness

import torch
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

# Random but reproducable
import torch

#Random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED) # for a notebook it usually work only for one code

random_tensor_C = torch.rand(3, 4)
torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3, 4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)

"""# GPU, CPU, cuda"""

#Running tensors and Pytorch objects on the Gpu (for faster computation)
#Gpu = faster computation on numbers, thanks to CUDA + NVIDIA hardware + Pytorch working behind the scenes to make everything hunky dory (good).

### Getting a GPU
#1. Easiest - Use google colab for free gpu (options to upgrade as well)
#2. Use your own GPU takes a little bit of setup and requires the investment to purchase one
#3. Use cloud computing - GCP, AWS, Azure, these services allow you to rent computers on the cloud and access them

!nvidia-smi

### Check for GPU access with Pytorch
import torch
torch.cuda.is_available()

# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

#Count number of devices
torch.cuda.device_count()

## 3. Putting tensors and models on the GPU since they are very good in that
tensor = torch.tensor([1, 2, 3], device = "cpu")

print(tensor, tensor.device)

#Move tensor to GPU (if avaliable)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu # 0  means index of gpu i have one so its 0

### 4 .Moving tensors back to the CPU
#if tensor is on GPU, cant transform it to NumPy
#To fix the GPU tensor with Numpy issue, we can first set it to the CPU

tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu

"""# ftg"""

import torch
from torch import nn #nn contains all of pythorch building blocks
import matplotlib.pyplot as plt

#check version
torch.__version__

weight = 0.7
bias = 0.3

#create
start = 0
end = 1
step = 0.02
x = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * x + bias # this is what ML has to figure out

x[:10], y[:10]

len(x), len(y)

"""### Splitting data into training test sets and graph
(training set, test set)
"""

#create a train/test split
train_split = int(0.8 * len(x)) #80% of data used for training set, 20% for testing
x_train, y_train = x[:train_split], y[:train_split]
x_test, y_test = x[train_split:], y[train_split:]

len(x_train), len(y_train), len(x_test), len(y_test)

#Visualize
def plot_predictions(train_data=x_train,
                     train_labels = y_train,
                     test_labels=y_test,
                     test_data=x_test,
                     predictions = None):
  plt.figure(figsize=(10, 7))

  #Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s = 4, label = "Training")
  #Plot testing data in green
  plt.scatter(test_data, test_labels, c="g", s = 4, label = "Testing")

  #Predictions?
  if predictions is not None:
    #plot if exist
    plt.scatter(test_data, test_labels, c="r", s = 4, label = "Predictions")
  #Show the legend
  plt.legend(prop={"size": 14});

plot_predictions()

"""# Model"""

import torch
class LinearRegressionModel(nn.Module): #nn.Module are like building blocks Lego for models
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            requires_grad = True,
                                            dtype=torch.float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad = True,
                                         dtype=torch.float))
    #Forward method to define the computation method
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias # linear formula

#torch.nn - contains all of the buildings for computational graphs (a neural network is a graph in this case)
#torch.nn.Parameter - what parameters should our model try and learn, often a Pytorch layer from torch.nn will set these for us
#torch.nn.Module - the base class for all neural network modules, if you subclass it, you should overwrite forward()
#torch.optim - this where the optimizers in pytorch live, they will help with gradient descent
#def forward() - ann nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation

### Checking the content on the model
torch.manual_seed(43)

#Create an instance of a model (subclass)
model_0 = LinearRegressionModel()

#Check the parameters
list(model_0.parameters())

# List named parameters
model_0.state_dict()

weight, bias

### Making predictions of y_test on x_test
with torch.inference_mode(): #turns off gradient tracking, it uses less data to get predictions
  y_preds = model_0(x_test) # torch.no_grad() preety much does the same but worse

y_preds

plot_predictions(predictions=y_preds)

## Train model (loss,cost function | optimizer(takes the loss and adjusts values))
# it measures how wrong it is to the sure value
loss_fn = nn.L1Loss()

#Optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(params = model_0.parameters(),
                            lr=0.01) # learning rate you can set it how you feel (how much it changes the parameter each time to see which works the best)

###Training loop and a tesnting loop
#0. Loop through the data
#1. Forward pass
#2. Calculate the loss (compare to prediction)
#3. Optimizer zero grad
#4. Loss backward - move backwards through the netword to calculate the gradients
#5. Optimizer step = use the optimizer to adjust our model parameters to try and improve the loss

# An epoch is one loop through the data
torch.manual_seed(42)
epochs = 500
#Track different values
epoch_count = []
loss_values = []
test_loss_values = []

#Training
#0. Loop through the data
for epoch in range(epochs):
  #Set model to training mode
  model_0.train() #train mode in pytorch sets all paremeters that require gradients to gradients

  # 1. Forward pass
  y_pred = model_0(x_train)

  # 2.Calculate the loss
  loss = loss_fn(y_pred, y_train)
  #print(f"Loss: {loss}")

  #3. Optimizer zero grad
  optimizer.zero_grad()

  #4. Perform backpropagation on the loss with respect to the parameters of the model
  loss.backward()

  #5. Step the optimizer (perform gradient descent)
  optimizer.step() # by default the optimizer changes will accumulate through the loop so we have to zero them above the step 3 for the next iteration in the loop

  ###Testing
  model_0.eval() # turns off the gradient
  with torch.inference_mode(): #turns off gradieng tracking and a couple more things nehind the scenes
    # 1. Do the forward pass
    test_pred = model_0(x_test)

    # 2. Calculate the loss
    test_loss = loss_fn(test_pred, y_test)

  #Print what is going on
  if epoch % 10 == 0:
    epoch_count.append(epoch)
    loss_values.append(loss)
    test_loss_values.append(test_loss)
    print(f"Epoch: {epoch} | loss: {loss} | Test loss: {test_loss}")

    #print out model state
    print(model_0.state_dict())

import numpy as np

# Loss values are in pytorch but plt needs numpy so we convert

plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label="Train loss")
plt.plot(epoch_count, test_loss_values, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend()

with torch.inference_mode():
  y_preds_new = model_0(x_test)

model_0.state_dict()

weight, bias

plot_predictions(predictions=y_preds);

plot_predictions(predictions=y_preds_new);















